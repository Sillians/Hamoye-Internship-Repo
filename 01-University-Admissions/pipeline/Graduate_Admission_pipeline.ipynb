{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graduate Admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem:\n",
    "This Project was built with the purpose of helping students in shortlisting universities with their profiles. The predicted output gives them a fair idea about their chances for a particular university.\n",
    "\n",
    "\n",
    "### Objective: \n",
    "Using the supplied predictive variables (GRE score, TOEFL score, University Rating, etc) to predict the likelihood of admission of a new candidate.\n",
    "\n",
    "\n",
    "\n",
    "### Data :\n",
    "\n",
    "The dataset contains several parameters which are considered important during the application for Masters Programs.\n",
    "The parameters included are :\n",
    "\n",
    "- GRE Scores ( out of 340 )\n",
    "- TOEFL Scores ( out of 120 )\n",
    "- University Rating ( out of 5 )\n",
    "- Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n",
    "- Undergraduate GPA ( out of 10 )\n",
    "- Research Experience ( either 0 or 1 )\n",
    "- Chance of Admit ( ranging from 0 to 1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.0 keras==1.2.2 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Restart the kernel before you proceed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Kubeflow pipelines SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your notebook kernel after updating the kfp sdk\n",
    "!pip3 install kfp --upgrade\n",
    "!pip3 install kfp --upgrade --user\n",
    "!pip install -U kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Check if the install was successful:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after the pip install\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'University Admission notebook pipeline'        # Name of the experiment in the UI\n",
    "BASE_IMAGE = \"tensorflow/tensorflow:latest-gpu-py3\"    # Base image used for components in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp # the Pipelines SDK. \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/01-University-Admissions/data/out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='preprocess_op',\n",
    "    description='preprocessing function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def preprocess(data_path):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    from sklearn.model_selection import train_test_split  # splitting the data\n",
    "    \n",
    "    # Get data\n",
    "    DATA_PATH = \"https://raw.githubusercontent.com/HamoyeHQ/01-University-Admissions/master/data/\"\n",
    "\n",
    "    def load_admission_data(admission_path=DATA_PATH):\n",
    "        csv_path = os.path.join(admission_path, \"Admission_Predict_Ver1.1.csv\")\n",
    "        return pd.read_csv(csv_path)\n",
    "    \n",
    "    # load data from function\n",
    "    dataset = load_admission_data()\n",
    "    \n",
    "    # rename columns\n",
    "    dataset.rename(columns={'GRE Score':'Gre_Score','TOEFL Score':'TOEFL_Score','University Rating':'University_Rating',\n",
    "                                       'LOR ':'LOR',  'Chance of Admit ':'Chance_of_Admit'}, inplace = True)\n",
    "    \n",
    "    # drop unneccessary column\n",
    "    dataset = dataset.drop(['Serial No.'], axis=1)\n",
    "    \n",
    "    # split the data into X and y\n",
    "    X = dataset.drop(['Chance_of_Admit'], axis=1)  # predictor\n",
    "    y = dataset['Chance_of_Admit'] # target(label)\n",
    "    \n",
    "    # preprocess using powertransformer\n",
    "    pt = PowerTransformer(method='box-cox')\n",
    "    X_trans = pt.fit_transform(X)\n",
    "    \n",
    "    # retaining the previous columns\n",
    "    X = pd.DataFrame(X_trans, columns=X.columns)\n",
    "    \n",
    "    #creating dummy variables for University Rating and Research\n",
    "    X = pd.get_dummies(X, columns=['University_Rating', 'Research'], drop_first=True)\n",
    "    \n",
    "    # Split the data into training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    #output file to path\n",
    "    np.savez_compressed(f'{data_path}/preprocessed-data.npz', \n",
    "                       X_train=X_train,\n",
    "                       X_test=X_test,\n",
    "                       y_train=y_train,\n",
    "                       y_test=y_test)\n",
    "    print(\"Preprocessing Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function \n",
    "\n",
    "### Training the data with the BayesianRidge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='train_op',\n",
    "    description='training function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def train(data_path, model_file):\n",
    "    \n",
    "    # Install all the dependencies inside the function\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    \n",
    "    # import libraries for training\n",
    "    from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "    \n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    X_train = preprocessed_data['X_train']\n",
    "    y_train = preprocessed_data['y_train']\n",
    "    \n",
    "    # Instantiating the model \n",
    "    main_model = BayesianRidge()\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    main_model.fit(X_train,y_train)\n",
    "    \n",
    "    #Save the model to the designated \n",
    "    with open(f'{data_path}/{model_file}', 'wb') as file:\n",
    "        pickle.dump(main_model, file)\n",
    "        \n",
    "    print(\"Model Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_model = train(out_dir, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='predict_op',\n",
    "    description='prediction function for Graduate admission',\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step. \n",
    ")\n",
    "\n",
    "def predict(data_path, model_file):\n",
    "    \n",
    "    import pickle     # python object for (de)serialization\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # Evaluation metrics\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "    \n",
    "    # Load the saved BayesianRidge Regressor model\n",
    "    with open(f'{data_path}/{model_file}', 'rb') as file:\n",
    "        main_model = pickle.load(file)\n",
    "    \n",
    "    #load the preprocessed data\n",
    "    preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "    X_test = preprocessed_data['X_test']\n",
    "    y_test = preprocessed_data['y_test']\n",
    "    \n",
    "    #Evaluate the model and print the results\n",
    "    model_pred = main_model.predict(X_test)\n",
    "    \n",
    "    # print the RMSE\n",
    "    print('Model \\nRMSE score = {}' .format(np.sqrt(mean_squared_error(y_test, model_pred))))\n",
    "\n",
    "\n",
    "              \n",
    "    with open(f'{data_path}/model_result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {},\\nActual: {} \".format(model_pred, y_test))\n",
    "              \n",
    "    print('Prediction has be saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Evaluate the model and print the results\n",
    "#     print('Test Accuracy: %.3f' % reg_model.score(X_test, y_test))\n",
    "    \n",
    "#     # Save the model \n",
    "#     model_filename = \"model_file.pkl\"\n",
    "#     with open(f'{data_path}/{model_filename}', 'wb') as file:\n",
    "#         pickle.dump(reg_model, file)\n",
    "    \n",
    "    \n",
    "#     #Save the test_data as a pickle file to be used by the predict component.\n",
    "#     with open(f'{data_path}/test_data', 'wb') as f:\n",
    "#         pickle.dump((X_test,  y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(out_dir, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocess, train and predict lightweight components.\n",
    "preprocess_op = comp.func_to_container_op(preprocess, base_image=BASE_IMAGE)\n",
    "train_op = comp.func_to_container_op(train , base_image=BASE_IMAGE)\n",
    "predict_op = comp.func_to_container_op(predict , base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain-specific language \n",
    "@dsl.pipeline(\n",
    "    name='University Admission',\n",
    "    description='End-to-end training to predict the likelihood of admission of a new candidate.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def graduate_admission_container_pipeline(\n",
    "    data_path: str,\n",
    "    model_file: str\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"volume_creation\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create graduate admission preprocessing component\n",
    "    admission_preprocessing_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "     # Create graduate admission training component.\n",
    "    admission_training_container = train_op(data_path, model_file) \\\n",
    "                                    .add_pvolumes({data_path: admission_preprocessing_container.pvolume})\n",
    "    \n",
    "    # Create graduate prediction component.\n",
    "    admission_predict_container = predict_op(data_path, model_file) \\\n",
    "                                    .add_pvolumes({data_path: admission_training_container.pvolume})\n",
    "    \n",
    "     # Print the result of the prediction\n",
    "    Graduate_admission_result_container = dsl.ContainerOp(\n",
    "        name=\"Admission prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: admission_predict_container.pvolume},\n",
    "        arguments=['head', f'{data_path}/model_result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Kubeflow Pipelines lets you group pipeline runs by Experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH='graduate_admission_predictor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = graduate_admission_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=EXPERIMENT_NAME\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,'{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcr.io/kubeflow-images-public/tensorflow-2.1.0-notebook-cpu:1.0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
